Classic, spam classification problem

#Can't upload a large dataset file size to git. Will include a link to these processed data later.
#But here are the findings.

#Data of 4 set all were processed into  below data structure in octave.
#Run ini to load all 4 data set.

#How to run
#Tinker with lr.m for running testing...

X.mat : The mail itself as a string 
y.mat : 1 for spam 0 for mail.

Algorithm proposition: Classification

X: features of email
y: 1 for spam, 0 for mail.


Dataset1
m = 5171 emails
y = 1499 spams (29%), 3672 mails (71%)

DATA
====================
Data pre-processing
-----------------
Get a mail into a single string with new
line and all, then with a function that it's input is

side note: If it contains "" it's better to change csv to tsv for better
recognition by the function.

#1
Using importdata()

#2
raw2data
Input
vector of patterns(Case insensitive) , string
Output
Vector of 1 and 0's denote match pattern and doesn't match

Structuring X
-------------------
X: we will try about 350-400 words in usually found in spam mail. Then try on the
data to see if it works or not.

Word choosing:
When looking at a bunch of spam email y = 1, with this piece of shell script

tr -c '[:alnum:]' '[\n*]' < spam.mat| sort | uniq -c | sort -nr | head  -10

this will prints out the frequencies of the word appearances descending order.
And here are the words that I choose:

691 will
669 by
613 can
603 more
587 www
587 here
551 any
549 if
520 information
506 only
485 get
483 please
474 email
471 price
469 us
370 now
311 pills
313 within
313 free
306 width
306 size
295 money
264 just
257 online
256 click
243 mail
239 prices
222 products
194 market
192 account
191 available
182 viragra
164 offer
139 performance
134 dollars
132 meds
125 long
123 sales
105 sale
103 drugs
102 shipping
103 china
98 pain
96 pay
94 work
93 risks
90 spam
87 cash
84 bank
82 transfer
82 growth
82 financial
82 download
80 risk
79 cheap
79 body
78 increase
77 sex
76 men
73 love
73 hotlist
71 pharmacy
69 natural
68 xanax
68 show
68 paid
67 large
67 images
66 hot
64 dealer
62 safe
62 rolex
62 fund
62 charge
61 quick
60 medications
60 marketing
60 fast
59 doctor
57 women
57 profile
57 gold
56 personal
56 huge
56 guarantee
54 penis
52 prize
52 dvd
51 card
50 girls
50 bill
49 popular
49 him
48 longer
48 interested
46 sexual
46 pill
46 big
44 payment
44 partner
40 dealers
39 wife
39 drug
38 store
38 lottery
38 deal
37 waste
37 savings
37 muscle
36 quickly
36 image
36 fun
35 treatment
35 female
34 medication
32 erections
31 premium
31 adult
30 privacy
30 hottlist
30 grow
30 dosage
30 banking
29 massive
28 unlimited
28 promotions
28 laptop
28 dose
28 entertainment
27 prozac
27 inches
27 husband
26 videos
26 pics
26 alone
26 aggressive
26 advertisement
25 women
25 watches
25 vlagra
25 tape
25 single
25 hundreds
25 cheapest
24 winners
24 smoking
24 pleasure
24 impotence
24 hair
24 advertising
23 successful
23 nigeria
23 israeli
23 girl
23 erection
23 enhanced
23 dollar
23 amazing
22 satisfaction
22 porn
22 photos
22 paypal
22 mortgage
21 winning
21 wild
21 feet
21 extremely
21 cum
21 bed
21 boost
20 together
20 lucky
20 dating
19 online
19 awesome
18 luck
18 discrete
18 discounts
18 dieting
18 cholesterol
18 boy
18 anytime
18 antivirus
18 absolutely
17 step
17 rocket
17 reviews
17 pharmaceutical
17 omega
17 mother
17 incredible
17 inch
17 hand
17 dysfunction
17 completely
17 collection
17 chinese
17 bigger
17 agra
17 adipex
16 zimbabwe
16 straight
16 pictures
16 payments
16 nice
16 loan
15 shove
15 redhead
15 promotion
15 profits
15 photo
15 married
15 enhance
15 debt
15 areas
14 wink
14 exciting
14 erectile
14 cockcrow
14 casino
14 beautiful
13 seeking
13 relationship
13 patients
13 attractive
12 sexy
12 saving
11 xanaax
11 weapons
11 virus
11 trusted
11 totally
11 topics
11 downloads
11 depressant
11 couple
10 webcam
10 sizes
10 diet
10 depression
10 delays
10 delay
10 college
10 church
10 christian
10 chat
10 cancer
9 viagr
9 therapy
9 medz
9 medicine
9 med
9 lump
9 intercourse
9 income
9 hottest
9 fuck
9 flirty
9 fifteen
9 ejaculation
9 depressants
9 casual
9 bra
9 attorney
9 absolute
8 violence
8 superior
8 sister
8 sexually
8 semen
8 relationships
8 pcs
8 orgasms
8 longines
8 loans
8 hardcore
8 hands
8 fucked
8 fertility
8 fancy
8 diabetes
8 cocks
8 cheating
8 caught
8 bucks
7 wealth
7 vlagr
7 virtual
7 shows
7 psa
7 proved
7 premature
7 plll
7 orgasm
7 lonely
7 lifetime
7 killing
7 intimacy
7 incest
7 impotent
7 horny
7 hookup
7 glory
7 ez
7 extreme
7 cam
7 bmw
7 awarded
6 wet
6 visa
6 testimonials
6 spank
6 reproduction
6 reliabie
6 relation
6 pussy
6 promo
6 promotional
6 promethe
6 pricess
6 pllls
6 pilis
6 penls
6 junk
6 johnson
6 illegal
6 iaagra
6 hotmail
6 herbal
6 guys
6 guaranteee
6 guaaraantees
6 grow
6 grew
6 gay
6 fitness
6 fits
6 filthy
6 downloading
6 dollars
6 cousin
6 cock
6 cllick
6 ciilick
5 virgin
5 toy
5 tocks
5 teens
5 teenagers
5 teenage
5 specialist
5 specializes
5 smuggling
5 smuggle
5 retail
5 retailers
5 plices
5 maturity
5 mastercard
5 mailbox
5 intimate
5 hairy
5 gucci
5 goldman
5 fastest
5 excited
5 erect
5 eighteen
5 dying
5 douse
5 cute
5 covering
5 capsule
5 butt

Shuffling
---------------------------------------------------
Using tb.mat in proccessed_data, which is 5171x(375+1) last collumn is y

Shuffle rows order, but keep collumn order.
Introducing randperm() function
If A is 
A=
    4 7 8 9
    3 3 5 7
    6 4 8 6
A(:,randperm(size(A,2))); will do the job

Similarly we can do it for rows, Just switch them around

tb_shuffled. mat is our shuffled data

Splitting data into: train, cross-validation, test
---------------------------------------------------
%3102 + 1035 + 1034 = 5171
%Train set 3102
X_train = X(1:3102,:);
y_train = y(1:3102);

%Cross validation 1035
X_cv = X(3103:4137,:);
y_cv= y(3103:4137);

%Test set 1034
X_test = X(4138:end,:);
y_test = y(4138:end);

%To save all of them
%Train set
save data/processed_data/X_train.mat X_train
save data/processed_data/y_train.mat y_train

%Cross validation set
save data/processed_data/X_cv.mat X_cv
save data/processed_data/y_cv.mat y_cv

%Test set
save data/processed_data/X_test.mat X_test
save data/processed_data/y_test.mat y_test

%To get rid of the numbering before putting into algorithm
X_train = X_train(:,2:end);
X_cv= X_cv(:,2:end);
X_test= X_test(:,2:end);

%Check the data again
size(X_train,1) + size(X_cv,1) + size(X_test,1) %Should be 5171
size(y_train,1) + size(y_cv,1) + size(y_test,1) %Should be 5171

isequal(X_train,X(1:3102,:)) %Should be 1
isequal(y_train,y(1:3102))

isequal(X_cv , X(3103:4137,:))
isequal(y_cv, y(3103:4137))

isequal(X_test , X(4138:end,:))
isequal(y_test , y(4138:end))

%If everything is OK, move on to algorithm step

ALGORITHM
==================
Setting up cost function and classification for the data
-----------------------------------------------------------
Run lr.m for fast algorithm 

Implementation note:
Flip the theta over from 1x(n+1) to (n+1)x1 before feeding it into cost function again

Learning curve and finding lambda:
Looks like the dianostic is fine, ideal lambda is about around 0.3~0.4. Plot the 
learning curve out, it's converging. And the cost of cross-validation set is not 
too big. this is promising. Going from 1-m is really long so the error is save in
algorithm/disanogistic error_cv and error_train is save here.

Predicting accuracy, precise and recall
-------------------------------------------
With everything inplace, setting here are the result with all three sets. This is
one run.

With lambda = 0.3, threshold of h_theta(x) is about 0.24.

Iteration   308 | Cost: 2.194589e-01

Train set
================
Accuracy: 92.714378%
Precision: 85.651214%
Recall: 88.990826%
F1 score: 0.872891
--------------------

Cross-validation set
================
Accuracy: 90.724638%
Precision: 82.200647%
Recall: 86.101695%
F1 score: 0.841060
--------------------

Test set
================
Accuracy: 90.135397%
Precision: 84.848485%
Recall: 84.337349%
F1 score: 0.845921
--------------------

Now this is ok, we need to find a different spam email set and try on that to see
wether or not this is just this data set bias. Technically it's not but, maybe one
person collects...And things. Lets find a different spam email dataset and see it's 
error. Sees how ours perform.

Yep it's shit. It's like 20% accuracy on the unseen test dataset here are the results.

Iteration   319 | Cost: 2.266768e-01

Train set
================
Accuracy: 92.005158%
Precision: 84.061135%
Recall: 88.302752%
F1 score: 0.861298
--------------------

Cross-validation set
================
Accuracy: 89.371981%
Precision: 78.996865%
Recall: 85.423729%
F1 score: 0.820847
--------------------

Test set
================
Accuracy: 88.781431%
Precision: 82.142857%
Recall: 83.132530%
F1 score: 0.826347
--------------------

Unseen test set
================
Accuracy: 19.724771%
Precision: 15.553199%
Recall: 87.824351%
F1 score: 0.264264
--------------------

Diagnosing unseen dataset
------------------------
If you look here, only 20% guessed correctly on the set of 3052: 501 spam, 2551 not spam
This is not good.

I have plotted the learning curve as X,y whole set and crossvalidation as this new
set, this unseen set. Error cost cv is more than 1. This is unacceptable. what
this is is overfitting problem.

My mistakes is this:
----------------
call data set A. I select most frequent words in spam in dataset A. Therefore, that
it will mostly fits to set A. Even if I split them out to train, cv, test. Mistake
here is selecting features in dataset and test that agaisnt the same dataset.

What we should do truely is get features that's you see, but test it against the set
that's not touched on.

To fix this, we take our new unseen set as cross validation, and find a separate
data set for test.

Now, our problem is high cross-validation cost in learning curve. That is overfitting
to our training dataset.

What are our options:
- Get more training example.
- Reduce features.
- Increasing lambda.(Tried, but..., not so good)

****So, we would get more training example. And refined our features.

*More training example*
------------------------
Getting beter, and more training example is good. I say, about around 10.000 15.000
Should be enough i presummed.


*Redesigning features to this bigger training example*
-------------------------------------------------------
Here we come to the phase of redesigning feature, that is selecting patterns that
will give this machine learning a higher rate of accuracy on both data set, and 
unseen data set. 

What comes to mind is this, what are spam for ? Advertising is the majority. Let's
focus on that, and any of the phishing, credit card scam, we will deal with it later.

Advertising in what area ?

Usually selling something, drills in each category then pick out the best words.
Hmm...These category is abou it we will stick with this and see if the algorithm 
increase in accuracy.

Relationship: Finding partner, sex, porn, cam, cheating, char.
Financial: Getting rich, stock, forex, broker, mimic investor, fake entreupenuer,
loan, e-comerce, drop shipping, amazon, ebay shit, currency.
Insurance: selling, buying.
Pyramid scheme: recruiting members.
Real estate: Buying, selling.
Pharmacy: pills, enhancements, fake medicine, cure incurable disease, drugs,
illegal drugs,
Law: Child support, divorce, suing somebody because of something.
Services: setting up websites, marketting, ...
Propositions: business propositions with no risk, no work.

In America an ad must contain this
"Any content that advertises or promotes a commercial product or service,
including content on a website operated for a commercial purpose.”

What we do is splitting this into 1 day a category, then slowly work through 
every category. We should get the patterns that's going to detects these spam emails.
And of course we are going to add in the mispelling after we run this.
--------------------------------------

The problem I have is dataset_1, I split them to train, validation, test 60%/20%/20%.
All is good, cross cost is low, train is also low, everything is in order. 
Learning curve is fine. Test set shows about 90% accuracy.

But then I use that model to predict X in dataset 2. Low and behold barely
50% accuracy. less than 30% precision. Very disapointing.

But after some thinking and digging. Makes me realize this. From this user on machine learning forum on stack overflow.

Start quote:"

This is similar to another question I answered regarding cross-validation and
test sets. The key concept to understand here is independent datasets. Consider
just two scenarios:

    If you have lot's of resources you would ideally collect one dataset and
train your model via cross-validation. Then you would collect another
completely independent dataset and test your model. However, as I said
previously, this is usually not possible for many researchers.

Now, if I am a researcher who isn't so fortunate what do I do? Well, you can
try to mimic that exact scenario:

    Before you do any model training you would take a split of your data and
leave it to the side (never to be touched during cross-validation). This is to
simulate that very same independent dataset mentioned in the ideal scenario
above. Even though it comes from the same dataset the model training won't take
any information from those samples (where with cross-validation all the data is
used). Once you have trained your model you would then apply it to your test
set, again that was never seen during training, and get your results. This is
done to make sure your model is more generalizable and hasn't just learned your
data.

"End quote

What I have done is do the second senerio. That yes, I only have 1 dataset then I
split it in 60%/20%/20%. But this doesn't work very well. 

If I use this, I will get a good result on the test set, but when get an entirely
independent set. I would get a really big error. Now that's not good.

Here is what I should do, strive
to do., have atleast 2 set. Or 3 sets or any number of sets but
1 will not be touch, 1 will be totally independent from other. No, let me get more
specific

Company A collects dataset 1
Company B collects dataset 2
Company C collects dataset 3

If you have 3 data set, you can use 2 of them, combined then shuffled
then split them 70%train/30%crossvalidation. Then the test set is the dataset 3. NO
TOUCHING DATASET 3, NO RElATION, TOTALLY INDEPENDENT. Or else you will get
a false reading.

In short, avoid having the test set split off from the dataset that you have cross
validation and training on. Try to find a entirely different dataset as
the test set to test your model.

Ideally at least 2 entirely different set must be available.
Dataset1: 70% training 30% cross-validation
Dataset2: 100% test set.

If it's too fucking bad and
you have only 1 dataset  then you must shuffle the hell out of the dataset.

------------------------
Observations
Given your model is not suffering from anything. It's it should be overfitting

*Diversifying data increase in accuracy
------------------------
With 4 datasets here are my findings, diversify your training data does significantly
gives a higher predictions rate. With a training data, here:

With set 4 with no relation, different from other datasets. here are it's ratings

Here it does a good job of predicting dataset 1 so there's nothing to say
v
Agaisnt set 1
================
Accuracy: 78.785535% (4074 out of 5171)
Precision: 70.221328% (Predict 1's: 994 and about 698 is correct)                       
Recall: 46.564376% (Got:698 correct out of actual 1's: 1499)                            
F1 score: 0.559968
General cost is: 0.720567
--------------------

Here is the case of low precision rate
Skewed data
v
Agaisnt set 2
================
Accuracy: 62.549148% (1909 out of 3052)
Precision: 25.718608% (Predict 1's: 1322 and about 340 is correct)                      
Recall: 67.864271% (Got:340 correct out of actual 1's: 501)                             
F1 score: 0.373012
General cost is: 0.850710
--------------------

Here is the case of low recall,guess wrong
v
Agaisnt set 3
================
Accuracy: 42.200241% (31827 out of 75419)
Precision: 62.945256% (Predict 1's: 25519 and about 16063 is correct)                   
Recall: 31.998645% (Got:16063 correct out of actual 1's: 50199)                         
F1 score: 0.424285

Now let's say we add about 100 elements or emails from each set to the train set and see if it does
changes anything.

This goes up in accuracy, recall, and precision
v
Agaisnt set 1
================
Accuracy: 82.266486% (4254 out of 5171)
Precision: 75.616197% (Predict 1's: 1136 and about 859 is correct) 
Recall: 57.304870% (Got:859 correct out of actual 1's: 1499)
F1 score: 0.651992
General cost is: 0.563727
--------------------

This too went up significantly in all 3
v
Agaisnt set 2
================
Accuracy: 82.929227% (2531 out of 3052)
Precision: 48.823529% (Predict 1's: 850 and about 415 is correct) 
Recall: 82.834331% (Got:415 correct out of actual 1's: 501)
F1 score: 0.614360
General cost is: 0.384701
--------------------

This too went up sinificantly from life 46% to 86% a jump in 40% DIFFERENCES,
therefore.
V
Agaisnt set 3
================
Accuracy: 86.384068% (65150 out of 75419)
Precision: 90.051757% (Predict 1's: 49848 and about 44889 is correct) 
Recall: 89.422100% (Got:44889 correct out of actual 1's: 50199)
F1 score: 0.897358
General cost is: Inf
--------------------

Further more is this, I just added 100 elements from each, now let's just add
1 in set 1 then we will see if it will increase anything.

Increase in everything, not suprised
V
Agaisnt set 1
================
Accuracy: 82.111777% (4246 out of 5171)
Precision: 75.716846% (Predict 1's: 1116 and about 845 is correct) 
Recall: 56.370914% (Got:845 correct out of actual 1's: 1499)
F1 score: 0.646272
General cost is: 0.605731
--------------------

Lowered the Precision 25->19, 67->69 increase recall
v
Agaisnt set 2
================
Accuracy: 46.395806% (1416 out of 3052)
Precision: 19.006008% (Predict 1's: 1831 and about 348 is correct) 
Recall: 69.461078% (Got:348 correct out of actual 1's: 501)
F1 score: 0.298456
General cost is: 1.250073
--------------------

Lowered 62->56 precision,Increase in recal 36->51
v
Agaisnt set 3
================
Accuracy: 38.718360% (29201 out of 75419)
Precision: 56.024333% (Predict 1's: 33041 and about 18511 is correct) 
Recall: 36.875237% (Got:18511 correct out of actual 1's: 50199)
F1 score: 0.444762
General cost is: 2.432238
--------------------

Here we can see that it will increase in recall, which is the correctness of our
algorithm which is awesome. But as we make room for new data, we will get lower
precision.

Now, let's see if we have an additional 100 elements in set 2

Agaisnt set 1
================
Accuracy: 82.131116% (4247 out of 5171)
Precision: 75.330396% (Predict 1's: 1135 and about 855 is correct)                      
Recall: 57.038025% (Got:855 correct out of actual 1's: 1499)                            
F1 score: 0.649203
General cost is: 0.574220
--------------------

Agaisnt set 2
================
Accuracy: 81.946265% (2501 out of 3052)
Precision: 46.995192% (Predict 1's: 832 and about 391 is correct)                       
Recall: 78.043912% (Got:391 correct out of actual 1's: 501)                             
F1 score: 0.586647
General cost is: 0.409908
--------------------

Agaisnt set 3
================
Accuracy: 64.794017% (48867 out of 75419)
Precision: 84.033275% (Predict 1's: 34741 and about 29194 is correct)                   
Recall: 58.156537% (Got:29194 correct out of actual 1's: 50199)                         
F1 score: 0.687403
General cost is: 1.326128
--------------------

This proves my point

In short, by diversifying your data you will increase in accuracy in predicting
unseen data. With just a small sample from an unseen dataset. You will significantly
increase your recall and precision.

Therefore, the best dataset is this. The best diversifed, like a portion from alot
of datasets as a training set. Then it will have a very high prediction rates.

Not overfitting that dataset, but try to overfit the diversified dataset is best.

Here is if we only take 100 elemts from dataset 4 2 1, then we will see the predictions
rate for 1 2 3 4

Set info:

**Data set 1**:
Enron email dataset

Taken from here
m = 5171 emails
y = 1499 spams (29%), 3672 mails (71%)

https://www.kaggle.com/venky73/spam-mails-dataset

**Data set 2**:
Spam assassin
https://www.kaggle.com/veleon/ham-and-spam-dataset

Spam : 501
Not spam: 2551
---------------
3052 in total

***Dataset 3***
TREC 2007 Public Corpus
https://plg.uwaterloo.ca/~gvcormac/treccorpus07/ 

Use with the TREC 2007 spam evaluation toolkit.
The corpus trec07p contains 75,419 messages:

    25220 ham
    50199 spam

***Dataset 4:
Ling-Spam corpus
https://www.aueb.gr/users/ion/data/lingspam_public.tar.gz

Spam: 481
Non spam: 2412
-------------
Total: 2893


Remember set 3 is not included in the training and crossvalidation. Therefore set 3 is the
test set.

Agaisnt set 1
================
Accuracy: 84.799845% (4385 out of 5171)
Precision: 78.004713% (Predict 1's: 1273 and about 993 is correct) 
Recall: 66.244163% (Got:993 correct out of actual 1's: 1499)
F1 score: 0.716450
General cost is: 0.414354
--------------------

Agaisnt set 2
================
Accuracy: 91.775885% (2801 out of 3052)
Precision: 70.903010% (Predict 1's: 598 and about 424 is correct) 
Recall: 84.630739% (Got:424 correct out of actual 1's: 501)
F1 score: 0.771611
General cost is: 0.198289
--------------------

Agaisnt set 3
================
Accuracy: 81.802994% (61695 out of 75419)
Precision: 91.427209% (Predict 1's: 44023 and about 40249 is correct) 
Recall: 80.178888% (Got:40249 correct out of actual 1's: 50199)
F1 score: 0.854344
General cost is: 0.655823
--------------------

Agaisnt set 4
================
Accuracy: 89.802973% (2598 out of 2893)
Precision: 64.808917% (Predict 1's: 628 and about 407 is correct) 
Recall: 84.615385% (Got:407 correct out of actual 1's: 481)
F1 score: 0.733995
General cost is: 0.203970
--------------------

This proves my point. Even similar to any combination of 2, 3 in set 1 2 3 4. The more accurate
the algorithm is.
